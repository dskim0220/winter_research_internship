import os
import torch
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

# 1. Blackwell(sm_121) í•˜ë“œì›¨ì–´ í˜¸í™˜ì„± ê°•ì œ ì„¤ì •
os.environ["TORCH_CUDA_ARCH_LIST"] = "12.1"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

# ëª¨ë¸ ê²½ë¡œ
path = "/home/logistics/.cache/huggingface/hub/models--Qwen--Qwen2.5-32B-Instruct-AWQ/snapshots/5c7cb76a268fc6cfbb9c4777eb24ba6e27f9ee6c"

print("--- ğŸš€ Blackwell(GB10) ê°€ì† ì»¤ë„ë¡œ Qwen-32B ë¡œë”© ì¤‘... ---")

try:
    # 2. ëª¨ë¸ ë¡œë“œ
    model = AutoAWQForCausalLM.from_quantized(
        path, 
        fuse_layers=True, 
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)

    # 3. NL2Opt ì—°êµ¬ìš© ì§ˆë¬¸
    prompt = "Linear Programming ëª¨ë¸ë§ ì‹œ, Gurobiì—ì„œ ë³€ìˆ˜ë¥¼ 'Integer'ì™€ 'Continuous'ë¡œ ì„¤ì •í•  ë•Œì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì¤˜."
    msg = [{"role": "user", "content": prompt}]
    
    text = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(text, return_tensors="pt").to("cuda")

    # 4. ë‹µë³€ ìƒì„± (Inference Mode ì‚¬ìš©)
    print("--- âœï¸ ë‹µë³€ ìƒì„± ì¤‘... ---\n")
    with torch.inference_mode():
        out = model.generate(**inputs, max_new_tokens=512)
    
    print(f"AI ë‹µë³€:\n{tokenizer.decode(out[0], skip_special_tokens=True).split('assistant')[-1].strip()}")

except Exception as e:
    print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
    print("íŒ: ì—¬ì „íˆ ì»¤ë„ ì—ëŸ¬ê°€ ë‚œë‹¤ë©´ ì„œë²„ì˜ NVIDIA ë“œë¼ì´ë²„ê°€ 570.xx ì´ìƒì¸ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.")